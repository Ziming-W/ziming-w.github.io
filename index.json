[{"content":"\u003ch2 id=\"purpose\"\u003e\u003cstrong\u003ePurpose\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eI was tasked to build a \u003cu\u003edata synchronization pipeline\u003c/u\u003e between ElasticSearch and PostgreSQL, in order to incrementally sync search data into ES. I didn\u0026rsquo;t want to handcraft too much so I looked into three existing middlewares: PGSync, Logstash+JDBC, and Elastic-Connector. This blog will discuss their pros and cons based on my actual experiments. Besides, in the last section, I will discuss other options to sync data and why I didn\u0026rsquo;t consider them.\u003c/p\u003e\n\u003cp\u003eThe conclusion is that\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eLogstash+JDBC\u003c/code\u003e is the best choice for my specific use cases despite it has some disadvantages.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ePGSync\u003c/code\u003e is pretty good middleware. But it has a bug. It also offers less flexibility and customization in terms of table relations.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eElastic-Connector\u003c/code\u003e for PostgreSQL is trash unless you only need very basic functionalities. It is more like a open-source framework for ppl to build own connectors on it.\u003c/li\u003e\n\u003cli\u003eRefer to last section for why I didn\u0026rsquo;t consider other options.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\r\n\u003cbr\u003e\r\n\u003ch2 id=\"what-i-need\"\u003e\u003cstrong\u003eWhat I need\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eI need two important features:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe middleware should be capable to \u003cu\u003eincrementally transforms and transfers data\u003c/u\u003e between ES and SQL. Some middlewares (e.g. some functionalities of Elastic-Connector) only support full-sync. Full-sync means sync everything by overriding the whole old ES index with the new one.\u003c/li\u003e\n\u003cli\u003eThe middleware should be able to perform \u003cu\u003ecomplex data transformations\u003c/u\u003e. Most importantly, nest/de-normalize SQL tables. This means nesting some child SQL tables into parent tables to make up a big JSON to store in ES.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eI don\u0026rsquo;t need exact real-time syncing for new changes. So syncing in intervals (e.g. every 20 seconds) is acceptable.\u003c/p\u003e\n\u003cbr\u003e\r\n\u003cbr\u003e\r\n\u003ch2 id=\"pgsync-vs-logstash\"\u003e\u003cstrong\u003ePGSync VS Logstash\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eLogstash is the most popular and widely-used middleware. It is developed and officially supported by ES team. However, I was initially attracted by PGSync  and decided to give it a try. It has many advantages over Logstash.\u003c/p\u003e\n\u003cp\u003eLogstash works by \u003ccode\u003eusing SQL to query database\u003c/code\u003e, extract changed data, then transform and send data to ElasticSearch. Instead, PGSync utilizes \u003ccode\u003elogical decoding\u003c/code\u003e feature of PostgreSQL and captures the change through the transaction logs. It doesn\u0026rsquo;t query database directly.\u003c/p\u003e\n\u003cp\u003eThis yields several advantages:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eKey points\u003c/th\u003e\n\u003cth\u003eLogstash\u003c/th\u003e\n\u003cth\u003ePGSync\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eImpact on DB\u003c/td\u003e\n\u003ctd\u003eHigh, since it queries DB directly using SQL\u003c/td\u003e\n\u003ctd\u003e‚úîÔ∏è Low, it passively captures data through logs\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;\u003c/td\u003e\n\u003ctd\u003e\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\u003c/td\u003e\n\u003ctd\u003e\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eTracking Column\u003c/td\u003e\n\u003ctd\u003eNeeded, Logstash uses it to check for changes. A timestamp is often used. All tables to be synced need to have this column. And they need to be updated when changes happen\u003c/td\u003e\n\u003ctd\u003e‚úîÔ∏è Not needed\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;\u003c/td\u003e\n\u003ctd\u003e\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\u003c/td\u003e\n\u003ctd\u003e\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eDeletion\u003c/td\u003e\n\u003ctd\u003e\u003cul\u003e\u003cli\u003eWe have to use soft deletion. Make \u003ccode\u003eis_deleted:Boolean\u003c/code\u003e column false when deleting. This is because if you hard delete a record, Logstash won\u0026rsquo;t know about it since it can\u0026rsquo;t see the tracking column any more. Unless extra efforts are made to ensure hard deletion consistency. \u003c/li\u003e \u003cli\u003e [Extra] However, in some cases soft deletion is the preferred approach for main DB anyway. \u003cul\u003e\u003cli\u003eFor example, recovery user data if he accidentally deleted something. Plus other similar reasons. \u003c/li\u003e \u003cli\u003e Besides, some articles mentioned massive hard deletion might make database index \u003ccode\u003efragmented\u003c/code\u003e (altho this is not the only cause) and affect performance. And if it gets too bad, the index should be reorganized or rebuilt. I did some readings on this topic. It doesn\u0026rsquo;t seem like it is a big problem for normal use cases. Check out \u003ca href=\"https://www.percona.com/blog/the-impacts-of-fragmentation-in-mysql/\"\u003e[\u003cu\u003ethis blog\u003c/u\u003e]\u003c/a\u003e on InnoDB engine\u003c/li\u003e \u003cli\u003eThe fun thing is I have literally came across two articles, the first one is \u0026ldquo;why you should do soft deletion\u0026rdquo;. The second one is \u0026ldquo;why soft deletion is bad\u0026rdquo;.üòÇ So I guess \u0026ldquo;soft deletion or not\u0026rdquo; is a case-by-case decision making.\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e\u003c/ul\u003e\u003c/td\u003e\n\u003ctd\u003e‚úîÔ∏è Can achieve hard/soft deletion\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"pgsync-300\"\u003e\u003cstrong\u003ePGSync (3.0.0)\u003c/strong\u003e\u003c/h2\u003e\n\u003ch3 id=\"a-bug\"\u003eA Bug\u003c/h3\u003e\n\u003cp\u003eI gave PGSync a try. One thing I gotta complain is the documentation is very broken and hard to understand. It is  made by a single developer, and has a small community.\u003c/p\u003e\n\u003cp\u003eThe version I used is 3.0.0. The syncing for a single table works pretty smooth. However, I encountered a bug when testing nested relationship. Incremental CREATE operation to child tables in a nested relationship won\u0026rsquo;t be reflected to ES. For example, suppose we nest \u003ccode\u003eReview\u003c/code\u003e tables inside \u003ccode\u003eMovie\u003c/code\u003e table. During the initial full-sync, everything works fine. However, in the subsequent syncing stages, creating a new \u003ccode\u003eReview\u003c/code\u003e table won\u0026rsquo;t be synced to ES.\u003c/p\u003e\n\u003cp\u003eI found a similar issue (\u003ca href=\"https://github.com/toluaina/pgsync/pull/493\"\u003ehttps://github.com/toluaina/pgsync/pull/493\u003c/a\u003e) raised. But his pull request doesn\u0026rsquo;t seem to fix the problem.\u003c/p\u003e\n\u003cp\u003eHere is a table summarized what works and what doesn\u0026rsquo;t, CUD stands for create/update/delete\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;\u003c/th\u003e\n\u003cth\u003eSingle Table\u003c/th\u003e\n\u003cth\u003eParent-Child table nesting\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eWill full-sync capture everything?\u003c/td\u003e\n\u003ctd\u003eYES\u003c/td\u003e\n\u003ctd\u003eYES\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eWill incremental CUD operation be captured and reflected to ES?\u003c/td\u003e\n\u003ctd\u003eYES\u003c/td\u003e\n\u003ctd\u003eif the CUD is made on parent table, YES; if UPDATE/DELETE is made on child table, YES; If the CREATE is made on child table. NO.\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"walk-around\"\u003eWalk-around\u003c/h3\u003e\n\u003cp\u003eThere is a walk-around to this issue. The primary issue lies in the fact that CREATE operations on the child table do not get properly reflected in ES. However, if we create a child table and then subsequently update an irrelevant field in parent table, all changes will be captured and synced to ES. This can solve the problem.\u003c/p\u003e\n\u003ch3 id=\"low-customization\"\u003eLow customization\u003c/h3\u003e\n\u003cp\u003eHowever, I soon realize another issue - the customization options are pretty limited. Our project is using generic foreign key to build relationships between some tables. Such relationship can\u0026rsquo;t be specified in PGSync\u0026rsquo;s descriptive JSON file. It is quite user-friendly but not as powerful as raw SQL, especially in terms of customizing relationships.\u003c/p\u003e\n\u003cbr\u003e\r\n\u003cbr\u003e\r\n\u003ch2 id=\"logstash--jdbc\"\u003e\u003cstrong\u003eLogstash + JDBC\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eNow, gotta go with Logstash. It has a big community and a lot of useful articles.\u003c/p\u003e\n\u003cp\u003eThose cons I mentioned earlier still hold. They caused some efforts to modify DB table and backend server logic to incorporate tracking column and soft deletion. But anyway I am happy with the strong customization capability provided by SQL and logstash\u0026rsquo;s internal filters.\u003c/p\u003e\n\u003cbr\u003e\r\n\u003cbr\u003e\r\n\u003ch2 id=\"elastic-connector\"\u003e\u003cstrong\u003eElastic-Connector\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.elastic.co/guide/en/enterprise-search/current/postgresql-connector-client-tutorial.html\"\u003e\u003cu\u003eOfficial guide on PostgreSQL-ES using connector\u003c/u\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/elastic/connectors/blob/8.11/docs/DOCKER.md\"\u003e\u003cu\u003eGeneral guide on how to run a connector in Docker\u003c/u\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eI also tried Elastic-Connector. After connecting ES to a connector, I can view it directly in Kibana GUI to schedule jobs and set up sync rules. This sounds pretty appealing to me.\u003c/p\u003e\n\u003cp\u003eIt is a pretty new middleware, and has literally 0 community resources. I only knew this middleware because it has top ranking on Google\u0026rsquo;s search result\u0026hellip;That was how it tricked me\u0026hellip; üò≠\u003c/p\u003e\n\u003ch3 id=\"problems\"\u003eProblems\u003c/h3\u003e\n\u003cp\u003eIt took me a long time to even set up a connector. Due to the lack of community resource, sometimes I gotta check the source code of it to debug.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFor example, it doesn\u0026rsquo;t even have a configuration option to disable SSL certification validation during the initial set up stages. I need to manually modify the source code to disable it and I hate doing this. Probably because this middleware is primarily made for native Elastic Cloud deployment\u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAfter I got everything working, I tested with some dummy data. There are \u0026ldquo;basic sync rules\u0026rdquo; and \u0026ldquo;advanced sync rules\u0026rdquo;.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBasic sync rules will work for both \u003ccode\u003eincremental sync\u003c/code\u003e and \u003ccode\u003efull sync\u003c/code\u003e. While advance sync rule only works for \u003ccode\u003efull sync\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eBasic sync rules contain some very basic matching conditions.\u003c/li\u003e\n\u003cli\u003eThe \u0026ldquo;table nesting\u0026rdquo; I want is within the advanced rule for PostgreSQL connector \u003ca href=\"https://www.elastic.co/guide/en/enterprise-search/current/connectors-postgresql.html#:~:text=that%20Elastic%20Deployment.-,Sync%20rules,-Basic%20sync%20rules\"\u003e\u003cu\u003eLink\u003c/u\u003e\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIt is definitely not capable for my use cases. I don\u0026rsquo;t see any advantage of using this one instead of Logstash. Probably the only good thing is it has GUI integration inside Kibana.\u003c/p\u003e\n\u003cbr\u003e\r\n\u003cbr\u003e\r\n\u003ch2 id=\"other-options\"\u003e\u003cstrong\u003eOther options!!!\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eSome other doable options:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePostgreSQL-\u0026gt;Debezium-\u0026gt;Kafka-\u0026gt;ES Connector/Logstash-\u0026gt;ES\u003c/strong\u003e: I didn\u0026rsquo;t take this into consideration for our project because it seems like a resource-intensive solution, since two more middlewares as used. But it looks very interesting to me. Debezium is a CDC (change data capture) tool and also uses PostgreSQL\u0026rsquo;s \u003ccode\u003elogical decoding\u003c/code\u003e feature. Plus this solution provides great flexibility, customization and stability. Maybe one day I will give it a try.\u003c/li\u003e\n\u003cli\u003eThird party data ETL tools such AirByte, Estuary\u0026hellip;\u003c/li\u003e\n\u003cli\u003eIn your backend, it is doable to perform create/update/delete to both the main SQL database and ES simultaneously. It is pretty flexible and customizable, but still sounds like a horrible idea to me. It is gonna mess up the backend code and performance. Imagine ES server is down, and a customer\u0026rsquo;s \u0026ldquo;add a comment\u0026rdquo; request got denied because SQL query succeeded but ES query failed\u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\r\n\u003cbr\u003e\r\n\u003ch2 id=\"end\"\u003eEnd\u003c/h2\u003e\n\u003cp\u003eThanks for reading! It is a long article and hopefully I didn\u0026rsquo;t make any mistakes. I am not an ES expert but I indeed spent quite some time to research these stuff. Please feel free to drop a comment below if I got anything wrong.\u003c/p\u003e\n","description":null,"image":"/elk.jpg","permalink":"/blogs/middlewares-es-sql/","title":"Middlewares for Data Syncing between ElasticSearch and PostgreSQL"},{"content":"\u003ch2 id=\"purpose\"\u003ePurpose\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eDeploy ElasticSearch (8.X), Kibana (8.X), Logstash (8.X), and PostgreSQL14 (optional) as native services on a AWS EC2 Ubuntu VM\u003c/li\u003e\n\u003cli\u003eAllow outside access for ES and Kibana.\u003c/li\u003e\n\u003cli\u003eSet up data syncing between ES and PostgreSQL, using Logstash + PostgreSQL JDBC.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e!!!! \u003cstrong\u003e\u003cu\u003eNotice that this is a very simple test deployment but not a production-ready deployment!\u003c/u\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eI only want to:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003etest availability of ELK stack\u003c/li\u003e\n\u003cli\u003e!!!! create some deployed search APIs for my frontend colleagues to call (through a backend server as the proxy), making their development easier\u003c/li\u003e\n\u003c/ol\u003e\n\u003cbr\u003e\r\n\u003cbr\u003e\r\n\u003ch2 id=\"key-ideas\"\u003eKey Ideas\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eExpose port 9200 for ElasticSearch\u0026rsquo;s programmatic interface. Your backend server can use ElasticSearch\u0026rsquo;s REST APIs here.\u003c/li\u003e\n\u003cli\u003eExpose port 5601 for Kibana\u0026rsquo;s GUI admin interface. You can access it in your local machine\u0026rsquo;s browser.\u003c/li\u003e\n\u003cli\u003eDeploy Logstash for data syncing between ES and PostgreSQL.\u003c/li\u003e\n\u003cli\u003eI also included an extra section for deploying PostgreSQL. It is optional if you have a running and reachable DB already. I added this section cuz it is easier to set it up on the VM instead of AWS RDS for my testing purpose.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\r\n\u003cbr\u003e\r\n\u003ch2 id=\"why-this-guide\"\u003eWhy this Guide?\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eI initially followed \u003cu\u003e\u003ca href=\"https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elastic-stack-on-ubuntu-22-04\"\u003e[\u003cstrong\u003ethis guide\u003c/strong\u003e]\u003c/a\u003e\u003c/u\u003e. However, it uses Version 7.X, which has different security settings than the latest 8.X. So Additional settings need to be configured for 8.X. Besides, this guides use Nginx as reverse proxy to handle all the traffic. But I want to skip this step and expose ES and Kibana directly to public.\u003c/li\u003e\n\u003cli\u003eElasticSearch\u0026rsquo;s official guide can be found \u003ca href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/deb.html\"\u003e[\u003cstrong\u003ehere\u003c/strong\u003e]\u003c/a\u003e. But it is sort of incomplete and contains redundant information.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\r\n\u003cbr\u003e\r\n\u003ch2 id=\"my-ec2-vm\"\u003eMy EC2 VM\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eSystem: Ubuntu 22.04.3 LTS (GNU/Linux 6.2.0-1018-aws x86_64)\u003c/li\u003e\n\u003cli\u003eMachine: t3.large, 2 vCPUs, 8GB memory. I tried 4GB and it is pretty laggy\u003c/li\u003e\n\u003cli\u003eFirewall: port 9200, 5601 open. Also open PostgreSQL\u0026rsquo;s default port if you want.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\r\n\u003cbr\u003e\r\n\u003ch2 id=\"es-and-kibana\"\u003eES and Kibana\u003c/h2\u003e\n\u003ch3 id=\"install-java\"\u003eInstall Java\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esudo apt update\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esudo apt install default-jre\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esudo apt install default-jdk\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003echeck for success:\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ejava -version\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ejavac -version\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"install-elasticsearch-8113\"\u003eInstall ElasticSearch (8.11.3)\u003c/h3\u003e\n\u003cp\u003eInstall signing key.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ecurl -fsSL https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elastic.gpg\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eecho \u0026quot;deb [signed-by=/usr/share/keyrings/elastic.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main\u0026quot; | sudo tee /etc/apt/sources.list.d/elastic-8.x.list\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInstall ES\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esudo apt update\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esudo apt install elasticsearch=8.11.3\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNow, start ES as a service, it is gonna take a while\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esudo systemctl start elasticsearch\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHealth check\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esudo systemctl status elasticsearch\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThen, we reset the password. It is easier to reset it than to find the default one in logs.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esudo /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe password should have been output to the terminal, copy it, then store it in an env variable:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eexport ES_PWD='\u0026lt;THE PASSWORD VALUE\u0026gt;'\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTest connection locally, a JSON should be returned\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ecurl -k -X GET -u elastic:$ES_PWD \u0026quot;https://localhost:9200\u0026quot;\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNow, we check if this port has been exposed to outside traffic successfully, open your \u003cstrong\u003elocal machine\u0026rsquo;s\u003c/strong\u003e terminal, run:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ecurl -k -X GET -u elastic:\u0026lt;THE PASSWORD VALUE\u0026gt; \u0026quot;https://\u0026lt;Remote VM IP\u0026gt;:9200\u0026quot;\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe same json as last step should be returned.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eVery important: We use -k to bypass the certification validation, so that the request can be sent even if our cert is self-signed in ES. Otherwise, the request will be rejected by CRUL. Using self-signed Certification will yield a signficant security threat (E.g. Man-in-the-middle attack). It is highly suggested to add a valid certificate as soon as possible\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eIf you failed somewhere, troubeshotting\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003echeck your VM firewall. Is 9200 open?\u003c/li\u003e\n\u003cli\u003eHave you messed up ES\u0026rsquo;s config file? Open \u003ccode\u003esudo nano /etc/elasticsearch/elasticsearch.yml\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003echeck that \u003ccode\u003ehttp.host\u003c/code\u003e should be \u003ccode\u003e0.0.0.0\u003c/code\u003e. This is the default setting.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enetwork.host\u003c/code\u003e should be commented or set as \u003ccode\u003elocalhost\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"install-kibana-8113\"\u003eInstall Kibana (8.11.3)\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eInstall Kibana \u003ccode\u003esudo apt-get update \u0026amp;\u0026amp; sudo apt-get install kibana=8.11.3\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eStart Kibana \u003ccode\u003esudo systemctl start kibana\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eHealth check \u003ccode\u003esudo systemctl status kibana\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNow we edit config to allow outside traffic, open the config file:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esudo nano /etc/kibana/kibana.yml\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eUNCOMMENT and/or EDIT following configs, when you are done, don\u0026rsquo;t forget to CTRL+S to save !\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e!!! Make sure you uncomment them by removing the #\u003c/li\u003e\n\u003cli\u003e!!! you can press CTRL+W to search. It is faster than scolling down\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eserver.host: 0.0.0.0\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eserver.port: 5601\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eelasticsearch.hosts: [\u0026quot;http://localhost:9200\u0026quot;]\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eRestart Kibana \u003ccode\u003esudo systemctl restart kibana\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eGo to \u003ccode\u003e\u0026lt;YOUR VM IP\u0026gt;:5601\u003c/code\u003e in your \u003cstrong\u003elocal machine\u0026rsquo;s browser\u003c/strong\u003e, you will get prompted to enter a token. If nothing shows up, wait for a while cuz it takes sometime for Kibana to start.\u003c/p\u003e\n\u003cp\u003eNow, we re-generate an enrolment token for kibana to use\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esudo /usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEnter this token.\u003c/p\u003e\n\u003cp\u003eThen, you will be prompted to enter a verification code\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003erun \u003ccode\u003ejournalctl -u kibana\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003ethen keep scolling down by pressing the down arrow on your keyboard. On the last page, you can find the code there.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFinally, you can use username/password to login, we have genereated the password for user \u003ccode\u003eelastic\u003c/code\u003e in earlier steps and stored it in an env variable. If you can\u0026rsquo;t find it, run \u003ccode\u003eecho $ES_PWD\u003c/code\u003e. If you have lost it, no worries, follow earlier steps to generate a new one.\u003c/p\u003e\n\u003ch3 id=\"uninstall\"\u003eUninstall\u003c/h3\u003e\n\u003cp\u003eDuring my setup, I frequently uninstall-reinstall ES and Kibana. Using \u003ccode\u003eapt remove\u003c/code\u003e, \u003ccode\u003epurge\u003c/code\u003e will not fully remove all configuration files for ES. Follow \u003ca href=\"https://stackoverflow.com/questions/45585881/how-to-remove-elasticsearch-from-ubuntu\"\u003e[\u003cstrong\u003ethis guide\u003c/strong\u003e]\u003c/a\u003e to manually \u003ccode\u003erm -rf\u003c/code\u003e relevant folders for a full clean-up.\u003c/p\u003e\n\u003cbr\u003e\r\n\u003cbr\u003e\r\n\u003ch2 id=\"postgresql\"\u003ePostgreSQL\u003c/h2\u003e\n\u003cp\u003eSkip this section if you have any available deployed PostgreSQL ready. Otherwise, you can do it in a VM and I personally find it easier.\u003c/p\u003e\n\u003cp\u003eWe also gonna expose it to outside traffic so that you can connect to it from your local machine and easily insert data when testing Logstash.\u003c/p\u003e\n\u003ch3 id=\"install\"\u003eInstall\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esudo apt update\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esudo apt install postgresql postgresql-contrib\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003estart service \u003ccode\u003esudo systemctl start postgresql\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003ehealth check \u003ccode\u003esudo systemctl status postgresql\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"configure\"\u003eConfigure\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003econnect to shell \u003ccode\u003esudo -u postgres psql\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSet up your DB, DB user, DB user password, grant permissions, etc.\u003c/li\u003e\n\u003cli\u003eNow, we edit config, open \u003ccode\u003esudo nano /etc/postgresql/\u0026lt;version\u0026gt;/main/postgresql.conf\u003c/code\u003e, make sure you replace the version number with the actual one. To find the version number, \u003ccode\u003esudo ls /etc/postgresql\u003c/code\u003e. Then, EDIT:\n\u003cul\u003e\n\u003cli\u003eopen to remote connection \u003ccode\u003elisten_addresses = '*'\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eadd your ip to trust list. Add this line \u003ccode\u003ehost    all             \u0026lt;DB USERNAME\u0026gt;          \u0026lt;YOUR IP ADDRESS\u0026gt;/32           md5\u003c/code\u003e. It is likely that you don\u0026rsquo;t have a static public ip from your ISP. Just google any \u0026ldquo;find my ip\u0026rdquo; tool and use that IP unless it is changed (which will likely to happen often, e.g. you turn off your PC). If you have made everything working, but one day later PostgreSQL produces some ip-blocked errors, then change this line to your new public ip.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e[Optional] Test connection from your \u003cstrong\u003elocal machine\u0026rsquo;s\u003c/strong\u003e shell\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003epsql -h \u0026lt;remote machine ip\u0026gt; -p \u0026lt;port\u0026gt; -U \u0026lt;DB USER\u0026gt; -d \u0026lt;DB NAME\u0026gt;\u003c/code\u003e to make sure you can connect to it locally.\u003c/li\u003e\n\u003cli\u003eThen, you can use any DB admin GUI on your local PC e.g. DBeaver to play around and insert data when testing Logstash.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\r\n\u003cbr\u003e\r\n\u003ch2 id=\"logstash\"\u003eLogstash\u003c/h2\u003e\n\u003ch3 id=\"install-logstash\"\u003eInstall Logstash\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esudo apt install logstash\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"download-postgresql-jdbc\"\u003eDownload PostgreSQL jdbc\u003c/h3\u003e\n\u003cp\u003eI am connecting PostgreSQL to ES so I need JDBC as a plugin to Logstash. If you are using MySQL, please download MySQL JDBC.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edownload the jdbc jar to your local machine. \u003ca href=\"https://jdbc.postgresql.org/download/\"\u003e[\u003cstrong\u003elink\u003c/strong\u003e]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003enow, we upload this jar to the VM. I used scp here.\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003escp -i \u0026lt;path to your private key on your local machine\u0026gt; \u0026lt;path to the jdbc jar file on your local machine\u0026gt; \u0026lt;VM username\u0026gt;@\u0026lt;VM ip\u0026gt;:\u0026lt;VM directory path to store the file\u0026gt;\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003enow, go your VM, you should be able to see this file\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"insert-dummy-data-in-postgresql\"\u003eInsert dummy data in postgresql\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003einsert dummy data in postgresql\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"creata-an-api-key-in-kibana\"\u003eCreata an api key in Kibana\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ego to kibana, create an api key in \u0026ldquo;Logstash\u0026rdquo; format. Copy and save it in your notepad.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"create-logstash-configuration-file\"\u003eCreate logstash configuration file\u003c/h3\u003e\n\u003cp\u003eNow, we create two configuration file for Logstash to use. \u003ccode\u003elogstash.conf\u003c/code\u003e is the main configuration file. I also created a \u003ccode\u003estatement.sql\u003c/code\u003e and let Logstash refer to this file for data syncing SQL. Or, you could put the SQL code inside \u003ccode\u003elogstash.conf\u003c/code\u003e, which also works.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eplace \u003ccode\u003estatement.sql\u003c/code\u003e in \u003ccode\u003esudo nano ~/statement.sql\u003c/code\u003e (or anywhere you like)\u003c/li\u003e\n\u003cli\u003eplace \u003ccode\u003elogstash.conf\u003c/code\u003e in \u003ccode\u003esudo nano ~/logstash.conf\u003c/code\u003e (or anywhere you like)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNow, we create \u003ccode\u003elogstash.conf\u003c/code\u003e. I believe that before trying to deploy to VM, you must have tried running logstash locally. There are a couple of things that need to be changed in this file accordingly. This file will vary from one person to another. Here is a checklist including what I changed. Make sure you thoroughly go through yours.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003emodify \u003ccode\u003ejdbc_driver_library\u003c/code\u003e to the absolute path where you stored the JDBC on VM\u003c/li\u003e\n\u003cli\u003emodify \u003ccode\u003ejdbc_connection_string\u003c/code\u003e if needed, make sure it points to your postgresql\u003c/li\u003e\n\u003cli\u003emodify \u003ccode\u003eapi_key\u003c/code\u003e to the one you just created earlier\u003c/li\u003e\n\u003cli\u003esince I used a seperate SQL file, I need to modify \u003ccode\u003estatement_filepath\u003c/code\u003e to the abosolute path of the file. I placed \u003ccode\u003estatement.sql\u003c/code\u003e in home. So it is \u003ccode\u003e/home/\u0026lt;User\u0026gt;/statement.sql\u003c/code\u003e. !!! Make sure you use absoluate path here!!!, not relative path like \u003ccode\u003e./statement.sql\u003c/code\u003e ‚ùå\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"run-logstash\"\u003eRun logstash\u003c/h3\u003e\n\u003cp\u003eHere, I run logstash through terminal directly instead of running it as a service. Because it is easier to specify extra arguments (I am using aggregate filter, so I need to limit number of worker thread to be 1 \u003ccode\u003e-w 1\u003c/code\u003e). If you are not using this aggregate filter then NVM.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNormal: \u003ccode\u003esudo /usr/share/logstash/bin/logstash -f ~/logstash.conf\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eFor me (aggregate filter) \u003ccode\u003esudo /usr/share/logstash/bin/logstash -f ~/logstash.conf -w 1\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe process should be running.\u003c/p\u003e\n\u003cp\u003eSince we are not running it as a service, we need to make sure the process is running after we close the shell, to achieve this, stop the process above, instead, do:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e nohup sudo /usr/share/logstash/bin/logstash -f ~/logstash.conf \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eFor me (aggregate filter) \u003ccode\u003e nohup sudo /usr/share/logstash/bin/logstash -f ~/logstash.conf -w 1 \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e(I also manually discarded stdout output)\u003c/p\u003e\n\u003cbr\u003e\r\n\u003cbr\u003e\r\n\u003ch2 id=\"good-jooooob\"\u003eGOOD JOOOOOB\u003c/h2\u003e\n\u003cp\u003eGOOOD JOOOOOOB.\u003c/p\u003e\n\u003cbr\u003e\r\n\u003cbr\u003e\r\n\u003ch2 id=\"what-next-for-a-better-set-up\"\u003eWhat Next for a better set up?\u003c/h2\u003e\n\u003cp\u003eSome of my thoughts.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e!!! Add more ES nodes and form a cluster to handle high concurrency\u003c/li\u003e\n\u003cli\u003eHave a valid SSL certificate\u003c/li\u003e\n\u003cli\u003eUse Nginx as reverse proxy and handle all SSL. Hide ES and Kibana.\u003c/li\u003e\n\u003cli\u003eOptimize security settings\u003c/li\u003e\n\u003cli\u003eetc\u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\r\n\u003cbr\u003e\r\n\u003ch2 id=\"extra-why-backend-as-a-proxy\"\u003eExtra: Why backend as a proxy?\u003c/h2\u003e\n\u003cp\u003eWhy we need to use backend server as a proxy to bridge frontend and ElasticSearch? Can\u0026rsquo;t we let frontend access ES directly to reduce network overhead? By:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003esetting up an frontend only API key with limited query permission\u003c/li\u003e\n\u003cli\u003elet ElasticSearch do the input validation/sanitization\u003c/li\u003e\n\u003cli\u003epre-construct search query associated with API key in ElasticSearch\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHere are some of the reasons why we can\u0026rsquo;t do it, there may be more:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eless secure than a dedicated backend server.\u003c/li\u003e\n\u003cli\u003esome search engine natively support this, e.g. TypeSense. ElasticSearch recently added support for this, with the help of \u0026ldquo;Search Application\u0026rdquo;. However,\n\u003cul\u003e\n\u003cli\u003e\u0026ldquo;Search Application\u0026rdquo; is still in beta.\u003c/li\u003e\n\u003cli\u003e\u0026ldquo;Search Application\u0026rdquo; is premium account only.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/search-application-security.html\"\u003e[\u003cstrong\u003eREAD MORE\u003c/strong\u003e]\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"end\"\u003eEnd\u003c/h2\u003e\n\u003cp\u003eThanks for reading!\u003c/p\u003e\n","description":null,"image":"/elk.jpg","permalink":"/blogs/deploy-elk-ubuntu/","title":"Deploy ELK Stack on a Remote Ubuntu VM"},{"content":"\u003cp\u003eSome good tech tutorials/blogs/sites that I found helpful in my study and programming.\u003c/p\u003e\n\u003ch2 id=\"backend\"\u003eBackend\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://www.youtube.com/watch?v=2PPSXonhIck\"\u003e[Tutorial]\u003c/a\u003e\u003c/strong\u003e \u003ca href=\"https://github.com/alex996/presentations/blob/master/auth.md\"\u003e[Notes]\u003c/a\u003e Authentication on the web. JWT, session, cookie, XSS, CSRF\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://www.youtube.com/watch?v=7hZXBrI2TjY\u0026amp;list=PL5aURjJ6mdUecuFvHqay0xZ0iN9SOe5nE\u0026amp;index=3\"\u003e[Tutorial]\u003c/a\u003e\u003c/strong\u003e System Design Interview with AWS. Design Youtube, Whatsapp, Twitter.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://www.bezkoder.com/\"\u003e[SITE]\u003c/a\u003e\u003c/strong\u003e Full stack web dev reference site. It has code example for any combo you want, e.g. Springboot+React, Django+Vue\u0026hellip; Not comprehensive tutorials but definitely sufficient to create a runnable tech stack rapidly for testing purposes.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://www.freecodecamp.org/news/what-exactly-is-client-side-rendering-and-hows-it-different-from-server-side-rendering-bd5c786b340d/\"\u003e[ARTICLE]\u003c/a\u003e\u003c/strong\u003e Client-side rendering VS server-side rendering\u003c/p\u003e\n\u003cbr\u003e\r\n\u003ch2 id=\"machine-learningdata\"\u003eMachine Learning/Data\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e[\u003ca href=\"https://blog.insightdatascience.com/bias-variance-tradeoff-explained-fa2bc28174c4\"\u003eARTICLE\u003c/a\u003e]\u003c/strong\u003e An in-depth explanation about bias and variance.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[\u003ca href=\"https://www.jeremyjordan.me/batch-normalization/\"\u003eARTICLE\u003c/a\u003e]\u003c/strong\u003e Importance of normalising data in neural networks.¬†\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://www.youtube.com/watch?v=aircAruvnKk\u0026amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\u0026amp;index=1\"\u003e[TUTORIAL]\u003c/a\u003e\u003c/strong\u003e The best series of introductory videos for neural networks. Mainly talked about MLP.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[\u003ca href=\"https://www.youtube.com/@firstprinciplesofcomputerv3258\"\u003eCOURSE\u003c/a\u003e]\u003c/strong\u003e University of Columbia\u0026rsquo;s full \u003cem\u003eComputer Vision\u003c/em\u003e course. Exceptionally well structured and delivered. Legendary professor.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[\u003ca href=\"https://distill.pub/2017/feature-visualization/\"\u003eARTICLE\u003c/a\u003e]\u003c/strong\u003e Feature visualization and how convolutional neural network learns\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[\u003ca href=\"https://convnetplayground.fastforwardlabs.com/#/models\"\u003eSITE\u003c/a\u003e]\u003c/strong\u003e A pre-trained CNN playground. You can use different pre-trained CNN as feature extractors. Then use extracted embeddings to perform Sementic Search - find another similar image within a dataset. There are many adjustable parameters, including datasets, pre-trained CNN, which layer you want to use, and distance metrics. Pretty fun to play around.\u003c/p\u003e\n\u003cbr\u003e\r\n\u003ch2 id=\"computer-networks\"\u003eComputer Networks\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://www.youtube.com/watch?v=ZMcmspNpkOw\u0026amp;list=PLMLm7-g0V0kdRIhd-qOtDSfG1tNAaQCBz\u0026amp;index=1\u0026amp;t=10s\"\u003e[TUTORIAL]\u003c/a\u003e\u003c/strong\u003e Computer network tutorials. I like his presentation style.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://github.com/alex/what-happens-when\"\u003e[ARTICLE]\u003c/a\u003e\u003c/strong\u003e What happens when you access a website? Explain all OSI layers\u0026rsquo; behaviour from the physical layer to the application layer.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://www2.tkn.tu-berlin.de/teaching/rn/animations/gbn_sr/\"\u003e[SITE]\u003c/a\u003e\u003c/strong\u003e Visualise and let you play with TCP\u0026rsquo;s retransmission. Helpful to the exam.\u003c/p\u003e\n\u003cbr\u003e\r\n\u003ch2 id=\"artifical-intelligence-the-subject\"\u003eArtifical Intelligence (The subject)\u003c/h2\u003e\n\u003cp\u003eThe Artificial Intelligence University subject (not AI in general sense). E.g. search algorithms, game agent, CSP, bayesian net\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://www.youtube.com/watch?v=UXW2yZndl7U\u0026amp;list=PL2bg-mBy9Lqi7Q14cOqOK7zVvR6Jfk4nA\u0026amp;index=4\"\u003e[TUTORIAL]\u003c/a\u003e\u003c/strong\u003e This professor is truly a legend. Helped me so much for the exam. You can find step-by-step demo for all search algorithms, MCTS and constrain-satisfication.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://raphsilva.github.io/utilities/minimax_simulator/#\"\u003e[SITE]\u003c/a\u003e\u003c/strong\u003e Minimax and alpha-beta pruning simulator. For the exam purpose, randomly make some trees by yourself and perform the pruning and backpropagations to practice.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://inst.eecs.berkeley.edu/~cs188/sp23/\"\u003e[COURSE]\u003c/a\u003e\u003c/strong\u003e Berkeley\u0026rsquo;s \u003cem\u003eArtificial Intelligence\u003c/em\u003e course.\u003c/p\u003e\n\u003cbr\u003e\r\n\u003ch2 id=\"distributed-systems\"\u003eDistributed Systems\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://pdos.csail.mit.edu/6.824/schedule.html\"\u003e[COURSE]\u003c/a\u003e\u003c/strong\u003e MIT\u0026rsquo;s \u003cem\u003eDistributed Systems\u003c/em\u003e. Most reputable yet challenging Distributed System course. It introduces many userful middlwares and technologies. But it heavily depends on your self-learning skills, as you need to read the original papers before watching the lectures. If you also feel sad taking Unimelb\u0026rsquo;s ill-designed trash course COMP90015 (Distributed Systems) that 40% of the contents overlap with COMP30023, 60% of the contents are too high-level and old AF, then consider self-studying this. Or you can use the syllabus here as a guide and watch other tutorials.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://www.youtube.com/watch?v=UEAMfLPZZhE\u0026amp;list=PLeKd45zvjcDFUEv_ohr_HdUFe97RItdiB\u0026amp;index=1\"\u003e[COURSE]\u003c/a\u003e\u003c/strong\u003e University of Cambridge\u0026rsquo;s \u003cem\u003eDistributed System\u003c/em\u003e. This one focuses more on the algorithms and theories instead of tools. It is similar to Unimelb\u0026rsquo;s \u003cem\u003eDistributed Algorithms\u003c/em\u003e subject.\u003c/p\u003e\n\u003cbr\u003e\r\n\u003ch2 id=\"cryptography\"\u003eCryptography\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://www.youtube.com/watch?v=z9bTzjy4SCg\"\u003e[TUTORIAL]\u003c/a\u003e\u003c/strong\u003e Tutorial for finite field (Galois field). This channel also has many good math tutorials. This video is especially helpful if you are a Unimelb student taking COMP90043 (Crytography and Security) - The subject content on the finite field part is so ill-designed. The lecture slides are stackings of formulas with no explanation. Plus if your lecturer is Udaya then\u0026hellip;good luck\u0026hellip; (Other than these, the subject is pretty good)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://www.youtube.com/playlist?list=PLBlnK6fEyqRgJU3EsOYDTW7m6SUmW6kII\"\u003e[COURSE]\u003c/a\u003e\u003c/strong\u003e \u003cem\u003eCrytography and Network Security\u003c/em\u003e full course. Might be helpful to your COMP90043 study. But I personally find the suggested textbook (\u003cem\u003eCrytography and Network Security - William Stallings\u003c/em\u003e) more useful for this subject. The subject syllabus follows the textbook exactly so if you don\u0026rsquo;t understand anything just refer to the textbook.\u003c/p\u003e\n\u003cbr\u003e\r\n\u003ch2 id=\"cs-self-learning--roadmap\"\u003eCS Self-Learning  RoadMap\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://csdiy.wiki/en/\"\u003e[RoadMap]\u003c/a\u003e\u003c/strong\u003e CS self-learning roadmap.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://roadmap.sh/\"\u003e[RoadMap]\u003c/a\u003e\u003c/strong\u003e Developer self-learning roadmap. E.g. backend developer, frontend developer, game developer\u0026hellip;\u003c/p\u003e\n\u003cbr\u003e\r\n\u003ch2 id=\"fun-stuff\"\u003eFun Stuff\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://www.youtube.com/watch?v=_3loq22TxSc\"\u003e[Video]\u003c/a\u003e \u003ca href=\"https://www.youtube.com/watch?v=sdkxWqsk17c\u0026amp;t=0s\"\u003e[Video]\u003c/a\u003e\u003c/strong\u003e Programming with Microsoft PowerPoint. Building a Turning Machine with it. üòß\n\u003cbr\u003e\u003c/p\u003e\n\u003cbr\u003e\r\n","description":null,"image":"/post-default.jpg","permalink":"/blogs/resource-sharing/","title":"Good Tech Tutorials/Blogs/Sites"}]